```bash
lsd@nuc:~/kube_kubectl$ kubectl apply -f namespace.yaml 
namespace/test created
lsd@nuc:~/kube_kubectl$ kubectl get ns
NAME              STATUS   AGE
kube-system       Active   26h
kube-public       Active   26h
kube-node-lease   Active   26h
default           Active   26h
test              Active   6s
lsd@nuc:~/kube_kubectl$ kubectl apply -f frontend.yaml 
pod/frontend created
lsd@nuc:~/kube_kubectl$ kubectl apply -f backend.yaml 
pod/backend created
lsd@nuc:~/kube_kubectl$ kubectl apply -f postgres.yaml 
statefulset.apps/postgres-db created
service/postgres-db-lb created
lsd@nuc:~/kube_kubectl$ kubectl get -n test all
NAME                READY   STATUS    RESTARTS   AGE
pod/frontend        1/1     Running   0          12m
pod/backend         1/1     Running   0          12m
pod/postgres-db-0   1/1     Running   0          3m11s

NAME                     TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
service/postgres-db-lb   LoadBalancer   10.152.183.105   <pending>     5432:30680/TCP   3m11s

NAME                           READY   AGE
statefulset.apps/postgres-db   1/1     3m11s
```



## Задание 1: проверить работоспособность каждого компонента
Для проверки работы можно использовать 2 способа: port-forward и exec. Используя оба способа, проверьте каждый компонент:
* сделайте запросы к бекенду;

```bash
lsd@nuc:~/kube_kubectl$ kubectl port-forward -n test pods/backend 8080:80
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
Handling connection for 8080
Handling connection for 8080
Handling connection for 8080
lsd@nuc:~$ curl 127.0.0.1:8080
Praqma Network MultiTool (with NGINX) - backend - 10.1.119.135
lsd@nuc:~$ curl 127.0.0.1:8080
Praqma Network MultiTool (with NGINX) - backend - 10.1.119.135
lsd@nuc:~$ curl 127.0.0.1:8080
Praqma Network MultiTool (with NGINX) - backend - 10.1.119.135
```

* сделайте запросы к фронту;

```bash
lsd@nuc:~/kube_kubectl$ kubectl port-forward -n test pods/frontend 8080:80
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
Handling connection for 8080
Handling connection for 8080
Handling connection for 8080
lsd@nuc:~$ curl 127.0.0.1:8080
Praqma Network MultiTool (with NGINX) - frontend - 10.1.119.134
lsd@nuc:~$ curl 127.0.0.1:8080
Praqma Network MultiTool (with NGINX) - frontend - 10.1.119.134
lsd@nuc:~$ curl 127.0.0.1:8080
Praqma Network MultiTool (with NGINX) - frontend - 10.1.119.134
```

* подключитесь к базе данных.

```bash
lsd@nuc:~/kube_kubectl$ kubectl port-forward -n test pods/postgres-db-0 8080:5432
Forwarding from 127.0.0.1:8080 -> 5432
Forwarding from [::1]:8080 -> 5432
Handling connection for 8080
```

```bash
lsd@nuc:~$ kubectl -n test get svc
NAME             TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
postgres-db-lb   LoadBalancer   10.152.183.105   <pending>     5432:30680/TCP   65m
frontend         ClusterIP      10.152.183.80    <none>        80/TCP           20m
backend          ClusterIP      10.152.183.131   <none>        80/TCP           20m
postgres-db      ClusterIP      10.152.183.120   <none>        5432/TCP         19m
lsd@nuc:~$ curl http://10.152.183.105:30680/
lsd@nuc:~$ curl http://10.152.183.80:80
Praqma Network MultiTool (with NGINX) - frontend - 10.1.119.134
lsd@nuc:~$ curl http://10.152.183.131:80
Praqma Network MultiTool (with NGINX) - backend - 10.1.119.135
```

## Задание 2: ручное масштабирование

При работе с приложением иногда может потребоваться вручную добавить пару копий. Используя команду kubectl scale, попробуйте увеличить количество бекенда и фронта до 3. Проверьте, на каких нодах оказались копии после каждого действия (kubectl describe, kubectl get pods -o wide). После уменьшите количество копий до 1.

```bash
lsd@nuc:~/kube_kubectl$ kubectl apply -f deployment_frontend.yaml 
deployment.apps/frontend created
lsd@nuc:~/kube_kubectl$ kubectl apply -f deployment_backend.yaml 
deployment.apps/backend created
lsd@nuc:~/kube_kubectl$ kubectl get -n test po
NAME                       READY   STATUS    RESTARTS   AGE
frontend                   1/1     Running   0          7h1m
backend                    1/1     Running   0          7h1m
postgres-db-0              1/1     Running   0          6h51m
frontend-c55978665-7lwch   1/1     Running   0          22s
backend-c55978665-9m4nb    1/1     Running   0          16s
```

```bash
lsd@nuc:~/kube_kubectl$ kubectl scale --replicas=3 -n test deployment frontend
deployment.apps/frontend scaled
lsd@nuc:~/kube_kubectl$ kubectl scale --replicas=3 -n test deployment backend
deployment.apps/backend scaled
lsd@nuc:~/kube_kubectl$ kubectl get -n test deployment
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
frontend   3/3     3            3           7m18s
backend    3/3     3            3           7m11s
lsd@nuc:~/kube_kubectl$ kubectl get -n test po
NAME                       READY   STATUS    RESTARTS   AGE
frontend                   1/1     Running   0          7h8m
backend                    1/1     Running   0          7h8m
postgres-db-0              1/1     Running   0          6h59m
frontend-c55978665-7lwch   1/1     Running   0          7m24s
backend-c55978665-9m4nb    1/1     Running   0          7m18s
frontend-c55978665-tbl67   1/1     Running   0          27s
frontend-c55978665-xqhcq   1/1     Running   0          27s
backend-c55978665-fwdpm    1/1     Running   0          16s
backend-c55978665-svcqb    1/1     Running   0          16s
```

```bash
lsd@nuc:~/kube_kubectl$ kubectl get -n test pods -o wide
NAME                       READY   STATUS    RESTARTS   AGE     IP             NODE   NOMINATED NODE   READINESS GATES
frontend                   1/1     Running   0          7h10m   10.1.119.134   nuc    <none>           <none>
backend                    1/1     Running   0          7h10m   10.1.119.135   nuc    <none>           <none>
postgres-db-0              1/1     Running   0          7h1m    10.1.119.139   nuc    <none>           <none>
frontend-c55978665-7lwch   1/1     Running   0          9m29s   10.1.119.141   nuc    <none>           <none>
backend-c55978665-9m4nb    1/1     Running   0          9m23s   10.1.119.142   nuc    <none>           <none>
frontend-c55978665-tbl67   1/1     Running   0          2m32s   10.1.119.144   nuc    <none>           <none>
frontend-c55978665-xqhcq   1/1     Running   0          2m32s   10.1.119.143   nuc    <none>           <none>
backend-c55978665-fwdpm    1/1     Running   0          2m21s   10.1.119.146   nuc    <none>           <none>
backend-c55978665-svcqb    1/1     Running   0          2m21s   10.1.119.145   nuc    <none>           <none>
```

```bash
lsd@nuc:~/kube_kubectl$ kubectl describe nodes
Name:               nuc
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=nuc
                    kubernetes.io/os=linux
                    microk8s.io/cluster=true
                    node.kubernetes.io/microk8s-controlplane=microk8s-controlplane
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 172.21.0.1/16
                    projectcalico.org/IPv4VXLANTunnelAddr: 10.1.119.128
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 23 Nov 2022 13:03:02 +0300
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  nuc
  AcquireTime:     <unset>
  RenewTime:       Thu, 24 Nov 2022 22:31:42 +0300
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Wed, 23 Nov 2022 13:07:35 +0300   Wed, 23 Nov 2022 13:07:35 +0300   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Thu, 24 Nov 2022 22:30:30 +0300   Wed, 23 Nov 2022 13:03:02 +0300   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Thu, 24 Nov 2022 22:30:30 +0300   Wed, 23 Nov 2022 13:03:02 +0300   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Thu, 24 Nov 2022 22:30:30 +0300   Wed, 23 Nov 2022 13:03:02 +0300   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Thu, 24 Nov 2022 22:30:30 +0300   Thu, 24 Nov 2022 19:01:14 +0300   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  192.168.0.253
  Hostname:    nuc
Capacity:
  cpu:                4
  ephemeral-storage:  196102748Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16234692Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  195054172Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16132292Ki
  pods:               110
System Info:
  Machine ID:                 0a1fb0086c6343cea047a52203fb957b
  System UUID:                cd35fe7b-3741-28d6-6c2b-f44d306d4cd6
  Boot ID:                    c24bd0c4-7f75-4c6e-b1c7-f5230f1d6d43
  Kernel Version:             5.4.0-126-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.6
  Kubelet Version:            v1.25.3
  Kube-Proxy Version:         v1.25.3
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                       ------------  ----------  ---------------  -------------  ---
  test                        frontend                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         7h11m
  test                        backend                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7h11m
  test                        postgres-db-0                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         7h2m
  kube-system                 hostpath-provisioner-5885cf6485-n9xbw      0 (0%)        0 (0%)      0 (0%)           0 (0%)         8h
  kube-system                 calico-node-cmtsf                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         33h
  default                     nfs-server-nfs-server-provisioner-0        0 (0%)        0 (0%)      0 (0%)           0 (0%)         8h
  kube-system                 calico-kube-controllers-c648b89bd-rckvx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         33h
  test                        frontend-c55978665-7lwch                   100m (2%)     200m (5%)   256Mi (1%)       512Mi (3%)     10m
  test                        backend-c55978665-9m4nb                    100m (2%)     200m (5%)   256Mi (1%)       512Mi (3%)     10m
  test                        frontend-c55978665-tbl67                   100m (2%)     200m (5%)   256Mi (1%)       512Mi (3%)     3m50s
  test                        frontend-c55978665-xqhcq                   100m (2%)     200m (5%)   256Mi (1%)       512Mi (3%)     3m50s
  test                        backend-c55978665-fwdpm                    100m (2%)     200m (5%)   256Mi (1%)       512Mi (3%)     3m39s
  test                        backend-c55978665-svcqb                    100m (2%)     200m (5%)   256Mi (1%)       512Mi (3%)     3m39s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                850m (21%)   1200m (30%)
  memory             1536Mi (9%)  3Gi (19%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-1Gi      0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:
  Type     Reason             Age                     From     Message
  ----     ------             ----                    ----     -------
  Warning  MissingClusterDNS  63s (x1055 over 3h30m)  kubelet  kubelet does not have ClusterDNS IP configured and cannot create Pod using "ClusterFirst" policy. Falling back to "Default" policy.
```

```bash
lsd@nuc:~/kube_kubectl$ kubectl describe -n test pod frontend-c55978665-7lwch
Name:             frontend-c55978665-7lwch
Namespace:        test
Priority:         0
Service Account:  default
Node:             nuc/192.168.0.253
Start Time:       Thu, 24 Nov 2022 22:21:03 +0300
Labels:           app=multitool
                  pod-template-hash=c55978665
Annotations:      cni.projectcalico.org/containerID: 303f6e8b104edc459c4d789668b1c9794fac631acbe3d7920d8b693f34295654
                  cni.projectcalico.org/podIP: 10.1.119.141/32
                  cni.projectcalico.org/podIPs: 10.1.119.141/32
Status:           Running
IP:               10.1.119.141
IPs:
  IP:           10.1.119.141
Controlled By:  ReplicaSet/frontend-c55978665
Containers:
  network-multitool:
    Container ID:   containerd://c44887150e18a37aaba4e7521014a15d1a0cd3d882a3c6425d7d6959c97cbdb1
    Image:          praqma/network-multitool:alpine-extra
    Image ID:       docker.io/praqma/network-multitool@sha256:5662f8284f0dc5f5e5c966e054d094cbb6d0774e422ad9031690826bc43753e5
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Thu, 24 Nov 2022 22:21:04 +0300
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     200m
      memory:  512Mi
    Requests:
      cpu:        100m
      memory:     256Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmmnf (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vmmnf:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason             Age                 From     Message
  ----     ------             ----                ----     -------
  Warning  MissingClusterDNS  13s (x13 over 11m)  kubelet  pod: "frontend-c55978665-7lwch_test(ea1d68a3-3588-4994-b4bd-95113848e9d4)". kubelet does not have ClusterDNS IP configured and cannot create Pod using "ClusterFirst" policy. Falling back to "Default" policy.
```

```bash
lsd@nuc:~/kube_kubectl$ kubectl scale --replicas=1 -n test deployment frontend backend
deployment.apps/frontend scaled
deployment.apps/backend scaled
lsd@nuc:~/kube_kubectl$ kubectl get -n test po
NAME                       READY   STATUS    RESTARTS   AGE
frontend                   1/1     Running   0          7h17m
backend                    1/1     Running   0          7h17m
postgres-db-0              1/1     Running   0          7h8m
frontend-c55978665-7lwch   1/1     Running   0          16m
backend-c55978665-fwdpm    1/1     Running   0          9m30s
```